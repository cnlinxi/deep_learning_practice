{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MengnanChen\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "refer to: https://blog.csdn.net/huachao1001/article/details/79120521\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''对于in_c个通道的输入图，如果需要通过卷积后输出out_c个feature map，\n",
    "那么总共需要in_c*out_c个卷积核'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入，shape=[c,h,w]，此处shape=[2,5,5]\n",
    "input_data=np.asarray([\n",
    "              [[1,0,1,2,1],\n",
    "               [0,2,1,0,1],\n",
    "               [1,1,0,2,0],\n",
    "               [2,2,1,1,0],\n",
    "               [2,0,1,2,0]],\n",
    "\n",
    "               [[2,0,2,1,1],\n",
    "                [0,1,0,0,2],\n",
    "                [1,0,0,2,1],\n",
    "                [1,1,2,1,0],\n",
    "                [1,0,1,1,1]],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#卷积核，shape=[in_c,k,k]=[2,3,3]\n",
    "weights_data=np.asarray([ \n",
    "               [[ 1, 0, 1],\n",
    "                [-1, 1, 0],\n",
    "                [ 0,-1, 0]],\n",
    "               [[-1, 0, 1],\n",
    "                [ 0, 0, 1],\n",
    "                [ 1, 1, 1]] \n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,h,w=input_data.shape\n",
    "cols=[]\n",
    "for i in range(c):\n",
    "    line=np.reshape(input_data[i],[h*w,1])\n",
    "    cols.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [0, 0],\n",
       "       [1, 2],\n",
       "       [2, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [2, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 2],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [2, 2],\n",
       "       [0, 1],\n",
       "       [2, 1],\n",
       "       [2, 1],\n",
       "       [1, 2],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [2, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [2, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2],\n",
       "        [0, 0],\n",
       "        [1, 2],\n",
       "        [2, 1],\n",
       "        [1, 1]],\n",
       "\n",
       "       [[0, 0],\n",
       "        [2, 1],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [1, 2]],\n",
       "\n",
       "       [[1, 1],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [2, 2],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[2, 1],\n",
       "        [2, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[2, 1],\n",
       "        [0, 0],\n",
       "        [1, 1],\n",
       "        [2, 1],\n",
       "        [0, 1]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape: [h,w,c]\n",
    "np.reshape(np.concatenate(cols,axis=1),[h,w,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "    s1,s2,s3=tensor.get_shape()\n",
    "    s1,s2,s3=int(s1),int(s2),int(s3)\n",
    "    return s1,s2,s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chw2hwc(chw_tensor):\n",
    "    '''\n",
    "    Tensorflow中定义的Tensor的shape为[n,h,w,c]，在这里将n设为1，batch_size设为1，\n",
    "    刚刚将输入定义为[c,h,w]，因此需要将[c,h,w]转为[h,w,c]\n",
    "    '''\n",
    "    c,h,w=get_shape(chw_tensor)\n",
    "    cols=[]\n",
    "    \n",
    "    for i in range(c):\n",
    "        line=tf.reshape(chw_tensor[i],[h*w,1])\n",
    "        cols.append(line)\n",
    "    \n",
    "    # [h*w,c]\n",
    "    inputs=tf.concat(cols,axis=1)\n",
    "    inputs=tf.reshape(inputs,[h,w,c])\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tensorflow使用卷积核时，使用的格式是[k,k,in_c,out_c]，\n",
    "上述定义卷积核时使用的格式是[in_c,k,k]。\n",
    "这里需要将[in_c,k,k] -> [k,k,in_c]。简化工作起见，设置out_c=1.\n",
    "因此可以直接对卷积核调用chw2hwc，再在最后一个维度上扩充即可形成[k,k,in_c,out_c]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hwc2chw(hwc_tensor):\n",
    "    h,w,c=get_shape(hwc_tensor)\n",
    "    cs=[]\n",
    "    for i in range(c):\n",
    "        # [h,w]->[1,h,w]\n",
    "        channel=tf.expand_dims(hwc_tensor[:,:,i],axis=0)\n",
    "        cs.append(channel)\n",
    "    # [1,h,w]...[1,h,w]->[c,h,w]\n",
    "    inputs=tf.concat(cs,axis=0)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_conv2d(inputs,weights):\n",
    "    conv=tf.nn.conv2d(inputs,weights,strides=[1,1,1,1],padding='SAME')\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_input=tf.constant(input_data,tf.float32)\n",
    "const_weights=tf.constant(weights_data,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 1., 2., 1.],\n",
       "        [0., 2., 1., 0., 1.],\n",
       "        [1., 1., 0., 2., 0.],\n",
       "        [2., 2., 1., 1., 0.],\n",
       "        [2., 0., 1., 2., 0.]],\n",
       "\n",
       "       [[2., 0., 2., 1., 1.],\n",
       "        [0., 1., 0., 0., 2.],\n",
       "        [1., 0., 0., 2., 1.],\n",
       "        [1., 1., 2., 1., 0.],\n",
       "        [1., 0., 1., 1., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(const_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0.,  1.],\n",
       "        [-1.,  1.,  0.],\n",
       "        [ 0., -1.,  0.]],\n",
       "\n",
       "       [[-1.,  0.,  1.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 1.,  1.,  1.]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(const_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tf.Variable(const_input,name='inputs')\n",
    "# [c,h,w]->[h,w,c], here: [2,5,5]->[5,5,2]\n",
    "inputs=chw2hwc(inputs)\n",
    "# [h,w,c]->[batch_size,h,w,c], here: [5,5,2]->[1,5,5,2]\n",
    "inputs=tf.expand_dims(inputs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=tf.Variable(const_weights,name='weights')\n",
    "# [in_c,k,k]->[k,k,in_c], here: [2,3,3]->[3,3,2]\n",
    "weights=chw2hwc(weights)\n",
    "# [k,k,in_c]->[k,k,in_c,out_c], here: [3,3,2,1]\n",
    "weights=tf.expand_dims(weights,axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size,h,w,c]\n",
    "conv=tf.nn.conv2d(inputs,weights,strides=[1,1,1,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 2.],\n",
       "         [ 0.],\n",
       "         [ 2.],\n",
       "         [ 4.],\n",
       "         [ 0.]],\n",
       "\n",
       "        [[ 1.],\n",
       "         [ 4.],\n",
       "         [ 4.],\n",
       "         [ 3.],\n",
       "         [ 5.]],\n",
       "\n",
       "        [[ 4.],\n",
       "         [ 3.],\n",
       "         [ 5.],\n",
       "         [ 9.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[ 3.],\n",
       "         [ 4.],\n",
       "         [ 6.],\n",
       "         [ 2.],\n",
       "         [ 1.]],\n",
       "\n",
       "        [[ 5.],\n",
       "         [ 3.],\n",
       "         [ 5.],\n",
       "         [ 1.],\n",
       "         [-2.]]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "# [batch_size,h,w,c]\n",
    "sess.run(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst=hwc2chw(conv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.,  0.,  2.,  4.,  0.],\n",
       "        [ 1.,  4.,  4.,  3.,  5.],\n",
       "        [ 4.,  3.,  5.,  9., -1.],\n",
       "        [ 3.,  4.,  6.,  2.,  1.],\n",
       "        [ 5.,  3.,  5.,  1., -2.]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [c,h,w]\n",
    "sess.run(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################使用numpy实现conv2d###########################\n",
    "\n",
    "input_data=[\n",
    "              [[1,0,1,2,1],\n",
    "               [0,2,1,0,1],\n",
    "               [1,1,0,2,0],\n",
    "               [2,2,1,1,0],\n",
    "               [2,0,1,2,0]],\n",
    "\n",
    "               [[2,0,2,1,1],\n",
    "                [0,1,0,0,2],\n",
    "                [1,0,0,2,1],\n",
    "                [1,1,2,1,0],\n",
    "                [1,0,1,1,1]] \n",
    "]\n",
    "\n",
    "weights_data=[ \n",
    "               [[ 1, 0, 1],\n",
    "                [-1, 1, 0],\n",
    "                [ 0,-1, 0]],\n",
    "               [[-1, 0, 1],\n",
    "                [ 0, 0, 1],\n",
    "                [ 1, 1, 1]] \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 5)\n",
      "(2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(input_data).shape)\n",
    "print(np.asarray(weights_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "]\n",
    "y=[\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "]\n",
    "x=np.asarray(x)\n",
    "y=np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  9],\n",
       "       [16, 25, 36],\n",
       "       [49, 64, 81]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conv(feature_map,kernel):\n",
    "    '''\n",
    "    feature map: [h,w]\n",
    "    kernel: [k,k]\n",
    "    return rs: [h,w]\n",
    "    输出一个卷积核组中的一个卷积核对一个通道中的feature map进行卷积后的结果\n",
    "    '''\n",
    "    h,w=feature_map.shape\n",
    "    k,_=kernel.shape\n",
    "    r=int(k/2)\n",
    "    # 定义填充0之后的feature map\n",
    "    padding_fm=np.zeros([h+2,w+2],np.float32)\n",
    "    # 保存计算结果\n",
    "    rst=np.zeros([h,w],np.float32)\n",
    "    # 将输入赋值到指定区域，即除了0填充的边界，剩下的区域\n",
    "    padding_fm[1:h+1,1:w+1]=feature_map\n",
    "    \n",
    "    # 以每个点为中心进行遍历，获得每一个通道上卷积核对feature map的卷积\n",
    "    # 该结果与其余通道的结果元素加，获得该组卷积核的feature map\n",
    "    for i in range(1,h+1):\n",
    "        for j in range(1,w+1):\n",
    "            roi=padding_fm[i-r:i+r+1,j-r:j+r+1]\n",
    "            rst[i-1][j-1]=np.sum(roi*kernel)\n",
    "            \n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv2d(inputs,weights):\n",
    "    c,h,w=inputs.shape\n",
    "    _,k,_=weights.shape  # [in_c,k,k]\n",
    "    outputs=[]\n",
    "    out_c=1  # 输出通道数设置为1\n",
    "    \n",
    "    for _ in range(out_c):\n",
    "        output_=np.zeros([h,w],np.float32)\n",
    "\n",
    "        for i in range(c):\n",
    "            # 当前通道feature map: [h,w]\n",
    "            feature_map=inputs[i]\n",
    "            # 当前通道kernel: [k,k]\n",
    "            w=weights[i]\n",
    "            rst=compute_conv(feature_map,w)\n",
    "            output_=output_+rst\n",
    "            \n",
    "        # 对out_c组卷积核执行卷积操作    \n",
    "        outputs.append(output_)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=np.asarray(input_data,np.float32)\n",
    "weights=np.asarray(weights_data,np.float32)\n",
    "rst=my_conv2d(inputs,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 2.,  0.,  2.,  4.,  0.],\n",
      "       [ 1.,  4.,  4.,  3.,  5.],\n",
      "       [ 4.,  3.,  5.,  9., -1.],\n",
      "       [ 3.,  4.,  6.,  2.,  1.],\n",
      "       [ 5.,  3.,  5.,  1., -2.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################MobileNet##############################\n",
    "'''\n",
    "refer to: https://blog.csdn.net/huachao1001/article/details/79171447\n",
    "\n",
    "MobileNet是针对移动端的优化，当需要压缩模型时，可以考虑使用MobileNet代替卷积\n",
    "\n",
    "相比于普通卷积，MobileNet采用的方法是，将卷积分解为两个操作：depthwise和pointwise\n",
    "pointwise：即普通的1*1卷积\n",
    "depthwise：\n",
    "- 普通的卷积：对于输入通道数为in_channel的feature map，在计算卷积时，\n",
    "输出的每个通道都需要对应in_channel个filter_size*filter_size卷积核参数。\n",
    "- MobileNet把每个输入feature map对应一个卷积核，输出通道数不变。\n",
    "真正对通道数做改变的是pointwise，也即是1*1卷积。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tensorflow中的depthwise操作，pointwise操作就是普通的conv1d\n",
    "\n",
    "tf.nn.depthwise_conv2d(\n",
    "    input,\n",
    "    filter,\n",
    "    strides,\n",
    "    padding,\n",
    "    rate=None,\n",
    "    name=None,\n",
    "    data_format=None\n",
    ")\n",
    "\n",
    "tf.nn.conv1d(\n",
    "    value,\n",
    "    filters,\n",
    "    stride,\n",
    "    padding,\n",
    "    use_cudnn_on_gpu=None,\n",
    "    data_format=None,\n",
    "    name=None\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入，shape=[c,h,w]=[2,5,5]\n",
    "input_data=[\n",
    "              [[1,0,1,2,1],\n",
    "               [0,2,1,0,1],\n",
    "               [1,1,0,2,0],\n",
    "               [2,2,1,1,0],\n",
    "               [2,0,1,2,0]],\n",
    "\n",
    "               [[2,0,2,1,1],\n",
    "                [0,1,0,0,2],\n",
    "                [1,0,0,2,1],\n",
    "                [1,1,2,1,0],\n",
    "                [1,0,1,1,1]],\n",
    "            ]\n",
    "\n",
    "# 卷积核，shape=[in_c,k,k]=[2,3,3]\n",
    "weights_data=[ \n",
    "               [[ 1, 0, 1],\n",
    "                [-1, 1, 0],\n",
    "                [ 0,-1, 0]],\n",
    "               [[-1, 0, 1],\n",
    "                [ 0, 0, 1],\n",
    "                [ 1, 1, 1]] \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "    s1,s2,s3=tensor.get_shape()\n",
    "    s1,s2,s3=int(s1),int(s2),int(s3)\n",
    "    return s1,s2,s3\n",
    "\n",
    "def chw2hwc(chw_tensor):\n",
    "    c,h,w=get_shape(chw_tensor)\n",
    "    cols=[]\n",
    "    \n",
    "    for i in range(c):\n",
    "        line=tf.reshape(chw_tensor[i],[h*w,1])\n",
    "        cols.append(line)\n",
    "        \n",
    "    # [h*w,c]\n",
    "    inputs=tf.concat(cols,axis=-1)\n",
    "    # [h*w,c]->[h,w,c]\n",
    "    inputs=tf.reshape(inputs,[h,w,c])\n",
    "    return inputs\n",
    "\n",
    "def hwc2chw(hwc_tensor):\n",
    "    h,w,c=get_shape(hwc_tensor)\n",
    "    cs=[]\n",
    "    for i in range(c):\n",
    "        # get [h,w]->[1,h,w] \n",
    "        channel=tf.expand_dims(hwc_tensor[:,:,i],axis=0)\n",
    "        cs.append(channel)\n",
    "    \n",
    "    # [1,h,w]...[1,h,w]->[c,h,w]\n",
    "    inputs=tf.concat(cs,axis=0)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_inputs=tf.constant(input_data,tf.float32)\n",
    "const_weights=tf.constant(weights_data,tf.float32)\n",
    "\n",
    "# [2,5,5]\n",
    "inputs=tf.Variable(const_inputs,name='input')\n",
    "# [2,5,5]->[5,5,2]\n",
    "inputs=chw2hwc(inputs)\n",
    "# [5,5,2]->[1,5,5,2]\n",
    "inputs=tf.expand_dims(inputs,axis=0)\n",
    "\n",
    "# [2,3,3]\n",
    "weights=tf.Variable(const_weights,name='weights')\n",
    "# [2,3,3]->[3,3,2]\n",
    "weights=chw2hwc(weights)\n",
    "[3,3,2,1]\n",
    "weights=tf.expand_dims(weights,axis=3)\n",
    "\n",
    "conv=tf.nn.depthwise_conv2d(inputs,filter=weights,strides=[1,1,1,1],padding='SAME')\n",
    "rst=hwc2chw(conv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1., -3.,  0.,  1., -2.],\n",
       "        [-1.,  3.,  1., -1.,  3.],\n",
       "        [ 1., -1.,  0.,  3., -2.],\n",
       "        [ 1.,  1.,  1., -2.,  1.],\n",
       "        [ 4.,  1.,  4.,  2., -1.]],\n",
       "\n",
       "       [[ 1.,  3.,  2.,  3.,  2.],\n",
       "        [ 2.,  1.,  3.,  4.,  2.],\n",
       "        [ 3.,  4.,  5.,  6.,  1.],\n",
       "        [ 2.,  3.,  5.,  4.,  0.],\n",
       "        [ 1.,  2.,  1., -1., -1.]]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=[\n",
    "              [[1,0,1,2,1],\n",
    "               [0,2,1,0,1],\n",
    "               [1,1,0,2,0],\n",
    "               [2,2,1,1,0],\n",
    "               [2,0,1,2,0]],\n",
    "\n",
    "               [[2,0,2,1,1],\n",
    "                [0,1,0,0,2],\n",
    "                [1,0,0,2,1],\n",
    "                [1,1,2,1,0],\n",
    "                [1,0,1,1,1]] \n",
    "]\n",
    "\n",
    "weights_data=[ \n",
    "               [[ 1, 0, 1],\n",
    "                [-1, 1, 0],\n",
    "                [ 0,-1, 0]],\n",
    "               [[-1, 0, 1],\n",
    "                [ 0, 0, 1],\n",
    "                [ 1, 1, 1]] \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conv(feature_map,kernel):\n",
    "    h,w=feature_map.shape\n",
    "    k,_=kernel.shape\n",
    "    r=int(k/2)\n",
    "    # 初始化填充后的map\n",
    "    padding_fm=np.zeros([h+2,w+2],np.float32)\n",
    "    # 保存计算结果\n",
    "    rst=np.zeros([h,w],np.float32)\n",
    "    # 将输入赋值到指定区域，即除了0填充的边界，剩下的区域\n",
    "    padding_fm[1:h+1,1:w+1]=feature_map\n",
    "    for i in range(1,h+1):  # i: 1~h\n",
    "        for j in range(1,w+1):  # j: 1~w\n",
    "            # 取出以当前点为中心的k*k区域\n",
    "            roi=padding_fm[i-r:i+r+1,j-r:j+r+1]\n",
    "            # 计算当前点的卷积，对k*k个点，点乘之后求和\n",
    "            rst[i-1][j-1]=np.sum(roi*kernel)\n",
    "        \n",
    "    return rst\n",
    "\n",
    "def my_depthwise(chw_inputs,chw_weights):\n",
    "    c,_,_=chw_inputs.shape\n",
    "    _,k,_=chw_weights.shape\n",
    "    outputs=[]\n",
    "    \n",
    "    for i in range(c):\n",
    "        # [h,w]\n",
    "        feature_map=chw_inputs[i]\n",
    "        # [k,k]\n",
    "        weight=chw_weights[i]\n",
    "        rst=compute_conv(feature_map,weight)\n",
    "        # 注意：和conv2d不同，此处不再是元素加，\n",
    "        # 而是将每一个通道卷积后的结果直接保存下来\n",
    "        outputs.append(rst)\n",
    "        \n",
    "    return np.asarray(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=np.asarray(input_data,np.float32)\n",
    "weights=np.asarray(weights_data,np.float32)\n",
    "rst=my_depthwise(inputs,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1., -3.,  0.,  1., -2.],\n",
       "        [-1.,  3.,  1., -1.,  3.],\n",
       "        [ 1., -1.,  0.,  3., -2.],\n",
       "        [ 1.,  1.,  1., -2.,  1.],\n",
       "        [ 4.,  1.,  4.,  2., -1.]],\n",
       "\n",
       "       [[ 1.,  3.,  2.,  3.,  2.],\n",
       "        [ 2.,  1.,  3.,  4.,  2.],\n",
       "        [ 3.,  4.,  5.,  6.,  1.],\n",
       "        [ 2.,  3.,  5.,  4.,  0.],\n",
       "        [ 1.,  2.,  1., -1., -1.]]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################反卷积#######################\n",
    "'''\n",
    "反卷积：反卷积后，下一层feature map的尺寸比上一层大。\n",
    "在卷积过程中，先将feature map内部以0填充，扩大尺寸，然后执行通常的卷积\n",
    "\n",
    "refer to: https://blog.csdn.net/huachao1001/article/details/79131814\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=[\n",
    "               [[1,0,1],\n",
    "                [0,2,1],\n",
    "                [1,1,0]],\n",
    "\n",
    "               [[2,0,2],\n",
    "                [0,1,0],\n",
    "                [1,0,0]],\n",
    "\n",
    "               [[1,1,1],\n",
    "                [2,2,0],\n",
    "                [1,1,1]],\n",
    "\n",
    "               [[1,1,2],\n",
    "                [1,0,1],\n",
    "                [0,2,2]]\n",
    "\n",
    "            ]\n",
    "weights_data=[ \n",
    "              [[[ 1, 0, 1],\n",
    "                [-1, 1, 0],\n",
    "                [ 0,-1, 0]],\n",
    "               [[-1, 0, 1],\n",
    "                [ 0, 0, 1],\n",
    "                [ 1, 1, 1]],\n",
    "               [[ 0, 1, 1],\n",
    "                [ 2, 0, 1],\n",
    "                [ 1, 2, 1]], \n",
    "               [[ 1, 1, 1],\n",
    "                [ 0, 2, 1],\n",
    "                [ 1, 0, 1]]],\n",
    "\n",
    "              [[[ 1, 0, 2],\n",
    "                [-2, 1, 1],\n",
    "                [ 1,-1, 0]],\n",
    "               [[-1, 0, 1],\n",
    "                [-1, 2, 1],\n",
    "                [ 1, 1, 1]],\n",
    "               [[ 0, 0, 0],\n",
    "                [ 2, 2, 1],\n",
    "                [ 1,-1, 1]], \n",
    "               [[ 2, 1, 1],\n",
    "                [ 0,-1, 1],\n",
    "                [ 1, 1, 1]]]  \n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "每个卷积核需要旋转180°再传入tf.nn.conv2d_transpose函数，因为tf.nn.conv2d_transpose内部\n",
    "会旋转180°，所以提前旋转再经过内部旋转，能够保证卷积核和我们使用的卷积核的数据排列一致\n",
    "\n",
    "我们定义的卷积核shape为[out_c,in_c,h,w]需要转化为Tensorflow反卷积中使用的[h,w,out_c,in_c]\n",
    "'''\n",
    "weights_np=np.asarray(weights_data,np.float32)\n",
    "# 将输入的每个卷积核旋转180°\n",
    "weights_np=np.rot90(weights_np,k=2,axes=(2,3))\n",
    "\n",
    "const_input=tf.constant(input_data,tf.float32)\n",
    "const_weights=tf.constant(weights_np,tf.float32)\n",
    "\n",
    "inputs=tf.Variable(const_input,tf.float32)\n",
    "const_weights=tf.constant(weights_np,tf.float32)\n",
    "\n",
    "inputs=tf.Variable(const_input,name='inputs')\n",
    "# [c,h,w]->[h,w,c]\n",
    "inputs=tf.transpose(inputs,perm=[1,2,0])\n",
    "# [h,w,c]->[batch_size,h,w,c]\n",
    "inputs=tf.expand_dims(inputs,axis=0)\n",
    "\n",
    "# [out_c,in_c,h,w]\n",
    "weights=tf.Variable(const_weights,name='weights')\n",
    "# [out_c,in_c,h,w]->[h,w,out_c,in_c]\n",
    "weights=tf.transpose(weights,perm=(2,3,0,1))\n",
    "\n",
    "#######################Tensorflow反卷积#######################\n",
    "# input_shape=[batch_size,h,w,c]\n",
    "input_shape=inputs.get_shape().as_list()\n",
    "# weights_shape=[h,w,out_c,in_c]\n",
    "weights_shape=weights.get_shape().as_list()\n",
    "output_shape=[input_shape[0],input_shape[1]*2,input_shape[2]*2,\n",
    "              weights_shape[2]]\n",
    "deconv=tf.nn.conv2d_transpose(inputs,weights,output_shape=output_shape,\n",
    "                             strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "deconv_val=sess.run(deconv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 4.,  4.],\n",
       "        [ 3.,  1.],\n",
       "        [ 6.,  7.],\n",
       "        [ 2.,  0.],\n",
       "        [ 7.,  7.],\n",
       "        [ 3.,  2.]],\n",
       "\n",
       "       [[ 4.,  5.],\n",
       "        [ 3.,  6.],\n",
       "        [ 3.,  0.],\n",
       "        [ 2.,  1.],\n",
       "        [ 7.,  8.],\n",
       "        [ 5.,  5.]],\n",
       "\n",
       "       [[ 8.,  8.],\n",
       "        [ 6.,  0.],\n",
       "        [ 8.,  8.],\n",
       "        [ 5., -2.],\n",
       "        [11., 14.],\n",
       "        [ 2.,  2.]],\n",
       "\n",
       "       [[ 3.,  3.],\n",
       "        [ 2.,  3.],\n",
       "        [ 7.,  9.],\n",
       "        [ 2.,  8.],\n",
       "        [ 3.,  1.],\n",
       "        [ 3.,  0.]],\n",
       "\n",
       "       [[ 5.,  3.],\n",
       "        [ 5.,  0.],\n",
       "        [11., 13.],\n",
       "        [ 3.,  0.],\n",
       "        [ 9., 11.],\n",
       "        [ 3.,  2.]],\n",
       "\n",
       "       [[ 2.,  3.],\n",
       "        [ 1.,  5.],\n",
       "        [ 4.,  3.],\n",
       "        [ 5.,  1.],\n",
       "        [ 4.,  3.],\n",
       "        [ 4.,  0.]]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "在TF中，Conv2d-CPU模式下目前仅支持NHWC格式，即[Number, Height, Weight, Channel]格式。\n",
    "Conv2d-GPU模式下以NCHW为主，但支持将NHWC转换为NCHW求解。\n",
    "下面的是Conv2d-CPU即NHWC格式\n",
    "'''\n",
    "\n",
    "deconv_val[0]  # batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 4.,  3.,  6.,  2.,  7.,  3.],\n",
       "         [ 4.,  3.,  3.,  2.,  7.,  5.],\n",
       "         [ 8.,  6.,  8.,  5., 11.,  2.],\n",
       "         [ 3.,  2.,  7.,  2.,  3.,  3.],\n",
       "         [ 5.,  5., 11.,  3.,  9.,  3.],\n",
       "         [ 2.,  1.,  4.,  5.,  4.,  4.]],\n",
       "\n",
       "        [[ 4.,  1.,  7.,  0.,  7.,  2.],\n",
       "         [ 5.,  6.,  0.,  1.,  8.,  5.],\n",
       "         [ 8.,  0.,  8., -2., 14.,  2.],\n",
       "         [ 3.,  3.,  9.,  8.,  1.,  0.],\n",
       "         [ 3.,  0., 13.,  0., 11.,  2.],\n",
       "         [ 3.,  5.,  3.,  1.,  3.,  0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nhwc->nchw\n",
    "sess.run(tf.transpose(deconv_val,perm=(0,3,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################numpy实现反卷积#####################\n",
    "\n",
    "def compute_conv(feature_map,kernel):\n",
    "    h,w=feature_map.shape\n",
    "    k,_=kernel.shape\n",
    "    r=int(k/2)\n",
    "    padding_fm=np.zeros([h+2,w+2],np.float32)\n",
    "    rst=np.zeros([h,w],np.float32)\n",
    "    padding_fm[1:h+1,1:w+1]=feature_map\n",
    "    for i in range(1,h+1):\n",
    "        for j in range(1,w+1):\n",
    "            roi=padding_fm[i-r:i+r+1,j-r:j+r+1]\n",
    "            rst[i-1][j-1]=np.sum(roi*kernel)\n",
    "    return rst\n",
    "\n",
    "def fill_zeros(inputs):\n",
    "    '''\n",
    "    内部填充0\n",
    "    '''\n",
    "    c,h,w=inputs.shape\n",
    "    rst=np.zeros([c,2*h+1,2*w+1],np.float32)\n",
    "    \n",
    "    for i in range(c):\n",
    "        for j in range(h):\n",
    "            for k in range(w):\n",
    "                rst[i,2*j+1,2*k+1]=inputs[i,j,k]\n",
    "    \n",
    "    return rst\n",
    "\n",
    "def my_deconv(inputs,weights):\n",
    "    out_c,in_c,h,w=weights.shape\n",
    "    out_h=2*h\n",
    "    out_w=2*w\n",
    "    outputs=[]\n",
    "    \n",
    "    for i in range(out_c):\n",
    "        w=weights[i]\n",
    "        output_=np.zeros([out_h,out_w],np.float32)\n",
    "        for j in range(in_c):\n",
    "            conv=compute_conv(inputs[j],w[j])\n",
    "            # 裁剪\n",
    "            output_=output_+conv[0:out_h,0:out_w]\n",
    "        outputs.append(output_)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=np.asarray(input_data,np.float32)\n",
    "inputs=fill_zeros(inputs)\n",
    "weights=np.asarray(weights_data,np.float32)\n",
    "deconv=my_deconv(inputs,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 4.,  3.,  6.,  2.,  7.,  3.],\n",
       "        [ 4.,  3.,  3.,  2.,  7.,  5.],\n",
       "        [ 8.,  6.,  8.,  5., 11.,  2.],\n",
       "        [ 3.,  2.,  7.,  2.,  3.,  3.],\n",
       "        [ 5.,  5., 11.,  3.,  9.,  3.],\n",
       "        [ 2.,  1.,  4.,  5.,  4.,  4.]], dtype=float32),\n",
       " array([[ 4.,  1.,  7.,  0.,  7.,  2.],\n",
       "        [ 5.,  6.,  0.,  1.,  8.,  5.],\n",
       "        [ 8.,  0.,  8., -2., 14.,  2.],\n",
       "        [ 3.,  3.,  9.,  8.,  1.,  0.],\n",
       "        [ 3.,  0., 13.,  0., 11.,  2.],\n",
       "        [ 3.,  5.,  3.,  1.,  3.,  0.]], dtype=float32)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "注意，下面是NCHW格式\n",
    "'''\n",
    "\n",
    "deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data2=np.zeros((10,10)).astype(np.float32)\n",
    "for x in range(4,7):\n",
    "    for y in range(3,6):\n",
    "        input_data2[y,x] = 1\n",
    "        \n",
    "weights_data2=np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MengnanChen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  3.,  2.,  5.,  3.],\n",
       "       [ 5.,  5., 12.,  7., 16.,  9.],\n",
       "       [ 4.,  4.,  9.,  5., 11.,  6.],\n",
       "       [11., 11., 24., 13., 28., 15.],\n",
       "       [ 7.,  7., 15.,  8., 17.,  9.],\n",
       "       [ 7.,  7., 15.,  8., 17.,  9.]], dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dilateconv = tf.nn.atrous_conv2d(np.expand_dims(np.expand_dims(input_data2,axis=0),axis=3),\n",
    "                                    np.expand_dims(np.expand_dims(np.rot90(weights_data2,2),axis=3),axis=4),\n",
    "                                    rate=2,padding=\"VALID\")\n",
    "sess.run(tf.squeeze(tf_dilateconv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "\n",
    "def my_dialateconv2d(inputs,weights):\n",
    "    in_c,h,w=inputs.shape\n",
    "    out_c,in_c,k,_=weights.shape\n",
    "    outputs=[]\n",
    "    \n",
    "    for i in range(out_c):\n",
    "        for j in range(in_c):\n",
    "            output_=None\n",
    "            kernel=weights[i][j]\n",
    "            input_data=inputs[j]\n",
    "            for t in range(k*2-1):\n",
    "                if t%2!=0:\n",
    "                    # 向卷积核中内部补0，注意：在实际操作中，并不增加卷积核的参数数量\n",
    "                    kernel=np.insert(kernel,t,values=0.,axis=0)\n",
    "                    kernel=np.insert(kernel,t,values=0.,axis=1)\n",
    "            \n",
    "            conved=convolve2d(input_data,kernel,mode='valid')\n",
    "            output_=conved if output_ is None else conved+output_\n",
    "             \n",
    "        outputs.append(output_)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=np.expand_dims(np.asarray(input_data2,np.float32),axis=0)\n",
    "weights=np.expand_dims(np.expand_dims(np.asarray(weights_data2,np.float32),axis=0),axis=0)\n",
    "my_dilateconv=my_dialateconv2d(inputs,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.,  1.,  3.,  2.,  5.,  3.],\n",
       "        [ 5.,  5., 12.,  7., 16.,  9.],\n",
       "        [ 4.,  4.,  9.,  5., 11.,  6.],\n",
       "        [11., 11., 24., 13., 28., 15.],\n",
       "        [ 7.,  7., 15.,  8., 17.,  9.],\n",
       "        [ 7.,  7., 15.,  8., 17.,  9.]], dtype=float32)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dilateconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
